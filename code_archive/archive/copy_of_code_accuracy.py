# -*- coding: utf-8 -*-
"""Copy of CODE accuracy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i-m23N4TpX6CkoeVS4Z5b3_awDM-RFla

## Initialize
"""

# Imports and defaults
import joblib
from joblib import Parallel, delayed
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt1
import numpy as np
from scipy.optimize import linprog
import time

mpl.style.use("classic")
mpl.rcParams["figure.figsize"] = [6, 4]

mpl.rcParams["axes.linewidth"] = 0.75
mpl.rcParams["errorbar.capsize"] = 3
mpl.rcParams["figure.facecolor"] = "w"
mpl.rcParams["grid.linewidth"] = 0.75
mpl.rcParams["lines.linewidth"] = 0.75
mpl.rcParams["patch.linewidth"] = 0.75
mpl.rcParams["xtick.major.size"] = 3
mpl.rcParams["ytick.major.size"] = 3

mpl.rcParams["pdf.fonttype"] = 42
mpl.rcParams["ps.fonttype"] = 42
mpl.rcParams["font.size"] = 10
mpl.rcParams["axes.titlesize"] = "medium"
mpl.rcParams["legend.fontsize"] = "medium"

import platform
print("python %s" % platform.python_version())
print("matplotlib %s" % mpl.__version__)
print("%d joblib CPUs" % joblib.cpu_count())

def linestyle2dashes(style):
  if style == "--":
    return (3, 3)
  elif style == ":":
    return (0.5, 2.5)
  else:
    return (None, None)

"""## Design Definition"""

# Optimal designs
def d_grad(X, V, p, return_grad=True):
  """Value of D-optimal objective and its gradient.

  X: n x d matrix of arm features
  V: prior design matrix
  p: distribution over n arms (design)
  """
  n, d = X.shape

  # inverse of the sample covariance matrix
  Xp = X * np.sqrt(p[:, np.newaxis])
  G = V + Xp.T.dot(Xp)
  invG = np.linalg.inv(G)

  # objective value (log det)
  _, obj = np.linalg.slogdet(invG)
  if return_grad:
    # gradient of the objective
    X2 = np.einsum("ki,kj->kij", X, X)
    M = np.einsum("kij,jl->kil", X2, invG)
    dp = - np.trace(M, axis1=-2, axis2=-1)
  else:
    dp = 0

  return obj, dp


def d_design(X, V=None, pi_0=None, num_iters=100, tol=1e-6, printout=True):
  """Frank-Wolfe algorithm for d-design optimization.

  X: n x d matrix of arm features
  V: prior design matrix
  pi_0: initial distribution over n arms (design)
  num_iters: maximum number of Frank-Wolfe iterations
  tol: stop when two consecutive objective values differ by less than tol
  """
  n, d = X.shape

  if V is None:
    V = np.zeros((d, d))
  V += 1e-6 * np.eye(d)  # avoiding singularity

  if pi_0 is None:
    # initial allocation weights are 1 / n and they add up to 1
    pi = np.ones(n) / n
  else:
    pi = np.copy(pi_0)

  # initialize constraints
  A_ub_fw = np.ones((1, n))
  b_ub_fw = 1

  # Frank-Wolfe iterations
  for iter in range(num_iters):
    # compute gradient at the last solution
    pi_last = np.copy(pi)
    last_obj, grad = d_grad(X, V, pi_last)

    if printout:
      print("%.4f" % last_obj, end=" ")

    # find a feasible LP solution in the direction of the gradient
    result = linprog(grad, A_ub_fw, b_ub_fw, bounds=[0, 1], method="highs")
    pi_lp = result.x
    pi_lp = np.maximum(pi_lp, 0)
    pi_lp /= pi_lp.sum()

    # line search in the direction of the gradient
    w = np.append(np.logspace(-10, 0, 21, base=2), 0)
    pi_ = np.outer(w, pi_lp) + np.outer(1 - w, pi_last)
    G = V[np.newaxis, :, :] + np.einsum("pi,ij,ik->pjk", pi_, X, X)
    _, obj = np.linalg.slogdet(G)
    best = np.argmax(obj)

    # update solution
    pi = w[best] * pi_lp + (1 - w[best]) * pi_last

    if np.abs(pi - pi_last).sum() < tol:
      break;
    iter += 1

  if printout:
    print()

  pi = np.maximum(pi, 0)
  pi /= pi.sum()
  return pi

"""## Define Environment"""

# Bandit environments and simulator
class LinBandit(object):
  """Linear bandit."""

  def __init__(self, X, theta, sigma=0.5, exclude_features = None, S = None, S_prime = None, high_feature_actions = 0):
    self.X = np.copy(X)  # K x d matrix of arm features
    self.K = self.X.shape[0]
    self.d = self.X.shape[1]
    self.theta = np.copy(theta)  # model parameter
    self.sigma = sigma  # reward noise

    self.mu = self.X.dot(self.theta)  # mean rewards of all arms
    self.best_arm = np.argmax(self.mu)  # optimal arm

    self.S = S
    self.S_prime = S_prime
    self.exclude_features = exclude_features
    self.high_feature_actions = high_feature_actions

    self.randomize()

  def randomize(self):
    # generate random rewards
    self.rt = self.mu + self.sigma * np.random.randn(self.K)

  def reward(self, arm):
    # instantaneous reward of the arm
    return self.rt[arm]

  def regret(self, arm):
    # instantaneous regret of the arm
    return self.rt[self.best_arm] - self.rt[arm]

  ### define interpretability accuracy ###
  def interpretability(self, est_means, act):

    # print(est_means, act, np.max(self.mu))
    eps = 0.05
    if abs(self.mu[act] - np.max(self.mu)) < eps:
      self.at = 0
    else:
      self.at = 1

    # self.at = np.linalg.norm(est_means - self.mu)

    return self.at

  ### define interpretability accuracy ###
  def interpretability_theta(self, theta):

    self.at = np.linalg.norm(theta - self.theta)**2

    return self.at

  def pregret(self, arm):
    # expected regret of the arm
    return self.mu[self.best_arm] - self.mu[arm]

  def print(self):
    return "Linear bandit: %d dimensions, %d arms" % (self.d, self.K)


def evaluate_one(Alg, params, env, n, period_size=1):
  """One run of a bandit algorithm."""

  # print("here-eval-one-0")
  alg = Alg(env, n, params)

  regret = np.zeros(n // period_size)

  acc = np.zeros(n // period_size)
  acc_theta = np.zeros(n // period_size)

  # print("here-eval-one-1")
  for t in range(n):
    # generate state
    env.randomize()

    # take action and update agent
    arm = alg.get_arm(t)
    alg.update(t, arm, env.reward(arm))

    # track performance
    regret_at_t = env.regret(arm)
    regret[t // period_size] += regret_at_t

    # print("here")
    acc_at_t = env.interpretability(alg.get_mu(), alg.get_arm(t))
    acc[t // period_size] += acc_at_t

    # print("here")
    acc_theta_at_t = env.interpretability_theta(alg.get_theta())
    acc_theta[t // period_size] += acc_theta_at_t

  return regret, acc, acc_theta, alg


def evaluate(Alg, params, env, n=1000, period_size=1, printout=True):
  """Multiple runs of a bandit algorithm."""
  if printout:
    print("Evaluating %s" % Alg.print(), end="")
  start = time.time()

  # print("here-0")
  num_exps = len(env)
  regret = np.zeros((n // period_size, num_exps))

  acc = np.zeros((n // period_size, num_exps))
  acc_theta = np.zeros((n // period_size, num_exps))

  alg = num_exps * [None]

  # print("here-1")

  output = Parallel(n_jobs=-1)(delayed(evaluate_one)(Alg, params, env[ex], n, period_size)
    for ex in range(num_exps))

  # output = [evaluate_one(Alg, params, env[ex], n, period_size) for ex in range(num_exps)]


  for ex in range(num_exps):
    regret[:, ex] = output[ex][0]
    acc[:, ex] = output[ex][1]
    acc_theta[:, ex] = output[ex][2]
    alg[ex] = output[ex][3]
  if printout:
    print(" %.1f seconds" % (time.time() - start))

  # print("here-2")
  if printout:
    total_regret = regret.sum(axis=0)
    print("Regret: %.2f +/- %.2f (median: %.2f, max: %.2f, min: %.2f)" %
      (total_regret.mean(), total_regret.std() / np.sqrt(num_exps),
      np.median(total_regret), total_regret.max(), total_regret.min()))

    total_acc = acc.sum(axis=0)
    print("Loss: %.2f +/- %.2f (median: %.2f, max: %.2f, min: %.2f)" %
      (total_acc.mean(), total_acc.std() / np.sqrt(num_exps),
      np.median(total_acc), total_acc.max(), total_acc.min()))

    total_acc_theta = acc_theta.sum(axis=0)
    print("Loss (theta): %.2f +/- %.2f (median: %.2f, max: %.2f, min: %.2f)" %
      (total_acc_theta.mean(), total_acc_theta.std() / np.sqrt(num_exps),
      np.median(total_acc_theta), total_acc_theta.max(), total_acc_theta.min()))

  return regret, acc, acc_theta, alg

"""## Define Algorithm"""

# Bandit algorithms
class LinBanditAlg:
  def __init__(self, env, n, params):
    self.env = env  # bandit environment that the agent interacts with
    self.K = self.env.K  # number of arms
    self.d = self.env.d  # number of features
    self.n = n  # horizon
    self.theta0 = np.zeros(self.d)  # prior mean of the model parameter
    self.Sigma0 = np.eye(self.d)  # prior covariance of the model parameter
    self.sigma = 0.5  # reward noise

    self.pi = np.zeros(self.K)
    self.num_pulls = np.zeros(self.K)

    # override default values
    for attr, val in params.items():
      if isinstance(val, np.ndarray):
        setattr(self, attr, np.copy(val))
      else:
        setattr(self, attr, val)

    # sufficient statistics
    self.Lambda = np.linalg.inv(self.Sigma0)
    self.B = self.Lambda.dot(self.theta0)

  def update(self, t, arm, r):
    # update sufficient statistics
    x = self.env.X[arm, :]
    self.Lambda += np.outer(x, x) / np.square(self.sigma)
    self.B += x * r / np.square(self.sigma)

  def get_mu(self, ):
    return self.mu

  def get_theta(self, ):
    return self.theta


class LinTS(LinBanditAlg):
  def get_arm(self, t):
    # linear model posterior
    Sigmahat = np.linalg.inv(self.Lambda)
    thetahat = Sigmahat.dot(self.B)

    # posterior sampling
    self.thetatilde = np.random.multivariate_normal(thetahat, Sigmahat)
    self.mu = self.env.X.dot(self.thetatilde)

    arm = np.argmax(self.mu)

    # self.theta = thetahat
    self.theta = self.thetatilde
    return arm

  @staticmethod
  def print():
    return "LinTS"


class LinUCB(LinBanditAlg):
  def __init__(self, env, n, params):
    LinBanditAlg.__init__(self, env, n, params)

    self.cew = self.confidence_ellipsoid_width(n)

  def confidence_ellipsoid_width(self, t):
    # Theorem 2 in Abassi-Yadkori (2011)
    # Improved Algorithms for Linear Stochastic Bandits
    delta = 1 / self.n
    L = np.amax(np.linalg.norm(self.env.X, axis=1))
    Lambda = np.square(self.sigma) * np.linalg.eigvalsh(np.linalg.inv(self.Sigma0)).max()  # V = \sigma^2 (posterior covariance)^{-1}
    R = self.sigma
    S = np.sqrt(self.d)
    width = np.sqrt(Lambda) * S + \
      R * np.sqrt(self.d * np.log((1 + t * np.square(L) / Lambda) / delta))
    return width

  def get_arm(self, t):
    # linear model posterior
    Sigmahat = np.linalg.inv(self.Lambda)
    thetahat = Sigmahat.dot(self.B)

    # UCBs
    invV = Sigmahat / np.square(self.sigma)  # V^{-1} = posterior covariance / \sigma^2
    self.mu = self.env.X.dot(thetahat) + self.cew * \
      np.sqrt((self.env.X.dot(invV) * self.env.X).sum(axis=1))

    arm = np.argmax(self.mu)

    #self.pi = np.zeros(self.env.K)
    self.num_pulls[arm] += 1
    self.pi = self.num_pulls/self.num_pulls.sum()
    self.theta = thetahat

    return arm

  @staticmethod
  def print():
    return "LinUCB"


class LinGreedy(LinBanditAlg):
  def __init__(self, env, n, params):
    self.epsilon = 0.05

    LinBanditAlg.__init__(self, env, n, params)

  def get_arm(self, t):
    self.mu = np.zeros(self.K)

    if np.random.rand() < self.epsilon * np.sqrt(self.n / (t + 1)) / 2:
      self.mu[np.random.randint(self.K)] = np.Inf
    else:
      theta = np.linalg.solve(self.Lambda, self.B)
      self.mu = self.env.X.dot(theta)

    arm = np.argmax(self.mu)

    theta = np.linalg.solve(self.Lambda, self.B)
    self.mu = self.env.X.dot(theta)

    self.num_pulls[arm] += 1
    self.pi = self.num_pulls/self.num_pulls.sum()
    self.theta = theta


    return arm

  @staticmethod
  def print():
    return "Linear e-greedy"


class LinExploreCommit(LinBanditAlg):
  def __init__(self, env, n, params):
    self.epsilon = 0.05

    LinBanditAlg.__init__(self, env, n, params)

  def get_arm(self, t):
    self.mu = np.zeros(self.K)

    if t <= np.round(self.epsilon * self.n):
      self.mu[np.random.randint(self.K)] = np.Inf
      if t == np.round(self.epsilon * self.n):
        self.theta = np.linalg.solve(self.Lambda, self.B)
    else:
      self.mu = self.env.X.dot(self.theta)

    arm = np.argmax(self.mu)

    self.num_pulls[arm] += 1

    self.pi = self.num_pulls/self.num_pulls.sum()
    self.theta = np.linalg.solve(self.Lambda, self.B)
    self.mu = self.env.X.dot(self.theta)

    return arm

  @staticmethod
  def print():
    return "Linear explore-then-commit"


class LinPhasedElim(LinBanditAlg):
  def __init__(self, env, n, params):
    self.delta = 0.05  # confidence interval failure probability
    self.reset_statistics = True

    LinBanditAlg.__init__(self, env, n, params)

    # Section 22 in "Bandit Algorithms"
    self.phase = 0
    ell = self.phase + 1
    self.remaining_rounds = int(np.ceil(2 * d * np.power(4, ell) * \
      np.log(self.K * ell * (ell + 1) / self.delta)))
    self.active_arms = np.arange(self.K)

    # optimal design
    self.pi = d_design(self.env.X, num_iters=100, printout=False)

    self.pi_active = d_design(self.env.X, num_iters=100, printout=False)

  def get_arm(self, t):
    if not self.remaining_rounds:
      # linear model posterior
      Sigmahat = np.linalg.inv(self.Lambda)
      thetahat = Sigmahat.dot(self.B)

      self.mu = self.env.X.dot(thetahat)

      # elimination
      ci = np.power(0.5, self.phase + 1)
      UCB = self.env.X.dot(thetahat) + ci
      LCB = self.env.X.dot(thetahat) - ci
      self.active_arms = self.active_arms[UCB[self.active_arms] > LCB[self.active_arms].max()]

      # initialize a new phase
      self.phase += 1
      ell = self.phase + 1
      self.remaining_rounds = int(np.ceil(2 * d * np.power(4, ell) * \
        np.log(self.K * ell * (ell + 1) / self.delta)))

      if self.reset_statistics:
        # sufficient statistics
        self.Lambda = np.linalg.inv(self.Sigma0)
        self.B = self.Lambda.dot(self.theta0)

      # optimal design
      if self.active_arms.size > 1:
        self.pi_active = d_design(self.env.X[self.active_arms, :], num_iters=100, printout=False)
        self.pi_active /= self.pi_active.sum()
      else:
        self.pi_active = np.ones(1)
    else:
      self.remaining_rounds -= 1




    arm = np.random.choice(self.active_arms, p=self.pi_active)

    self.num_pulls[arm] += 1
    self.pi = self.num_pulls/self.num_pulls.sum()
    Sigmahat = np.linalg.inv(self.Lambda)
    thetahat = Sigmahat.dot(self.B)
    self.mu = self.env.X.dot(thetahat)
    self.theta = thetahat
    return arm

  @staticmethod
  def print():
    return "Linear phased elimination"


class CODE(LinBanditAlg):
  def __init__(self, env, n, params):
    self.acquisition = "policy"
    self.delta = 0.05  # confidence interval failure probability

    LinBanditAlg.__init__(self, env, n, params)
    self.L = 1
    self.prior_effect = 1e5 * np.square(self.sigma)  # V = \sigma^2 (posterior covariance)^{-1}

    self.pi = d_design(self.env.X, num_iters=100, printout=False)
    self.pi_active = np.zeros(self.K)

  def confidence_ellipsoid_width(self, t):
    # Theorem 2 in Abassi-Yadkori (2011)
    # Improved Algorithms for Linear Stochastic Bandits
    R = self.sigma
    S = 0
    width = np.sqrt(self.prior_effect) * S + \
      R * np.sqrt(self.d * np.log((1 + t * np.square(self.L) / self.prior_effect) / self.delta))
    return width

  def get_arm(self, t):
    # linear model posterior
    Sigmahat = np.linalg.inv(self.Lambda)
    thetahat = Sigmahat.dot(self.B)

    self.mu = self.env.X.dot(thetahat)

    # elimination
    cew = self.confidence_ellipsoid_width(t)
    invV = Sigmahat / np.square(self.sigma)  # V^{-1} = posterior covariance / \sigma^2
    ci = cew * np.sqrt((self.env.X.dot(invV) * self.env.X).sum(axis=1))
    UCB = self.env.X.dot(thetahat) + ci
    LCB = self.env.X.dot(thetahat) - ci
    self.active_arms = np.flatnonzero(UCB > LCB.max())

    if self.acquisition == "action":
      # minimum variance action
      V = self.Lambda[np.newaxis, :, :] + np.einsum("ij,ik->ijk", self.env.X, self.env.X)
      _, logdetV = np.linalg.slogdet(V)
      best = np.argmax(logdetV[self.active_arms])
      arm = self.active_arms[best]
    elif self.acquisition == "policy":
      if self.active_arms.size > 1:
        # minimum D-optimal design policy
        num_iters = 2 * self.d
        self.pi_active = d_design(self.env.X[self.active_arms, :], self.Lambda, num_iters=num_iters, tol=1e-4, printout=False)
        best = np.random.choice(self.active_arms.size, p=self.pi_active)
        arm = self.active_arms[best]
      else:
        arm = self.active_arms[0]
    else:
      raise Exception("Unknown acquisition function in LinOD.")

    self.num_pulls[arm] += 1
    self.pi = self.num_pulls/self.num_pulls.sum()
    self.theta = thetahat

    return arm

  @staticmethod
  def print():
    return "CODE"

"""## Start main"""

## Creating bandit envs
def generate_bandits(num_runs, theta0, Sigma0):
  envs = []
  for run in range(num_runs):
    # sample model parameter
    theta = np.random.multivariate_normal(theta0, Sigma0)
    # sample arm features from a hypercubecube
    X = 2 * np.random.rand(K, d) - 1
    # initialize bandit environment
    envs.append(LinBandit(X, theta))

  return envs


d = 30  # number of features
# K = 100  # number of arms
K = 200  # number of arms
n = 5000  # horizon
# n = 5000  # horizon
# num_runs = 200  # number of random runs
num_runs = 48  # number of random runs

algs = [
  ("LinUCB", {}, "green", "-", "LinUCB"),
  ("LinTS", {}, "yellowgreen", "-", "LinTS"),
  ("LinGreedy", {}, "blue", "-", r"$\varepsilon$-greedy"),
  ("LinExploreCommit", {}, "cyan", "-", "EtC"),
  ("LinPhasedElim", {}, "orange", "-", "Elimination"),
  ("CODE", {}, "red", "-", "CODE")]

# prior distribution of the model parameter
theta0 = np.zeros(d)  # prior mean
Sigma0 = np.eye(d)  # prior covariance (this is standard deviation in the MAB)

# bandit environments
envs = generate_bandits(num_runs, theta0, Sigma0)

step = np.arange(1, n + 1)  # for plots
sube = (step.size // 10) * np.arange(1, 11) - 1

mpl.rcParams["font.size"] = 7
plt.figure(figsize=(3.4, 2.25))


regret_list = []
acc_list = []
acc_theta_list = []


# simulation
for alg in algs:
  # all runs for a single algorithm
  alg_class = globals()[alg[0]]
  regret, acc, acc_theta, _ = evaluate(alg_class, alg[1], envs, n)

  regret_list.append(regret)
  acc_list.append(acc)
  acc_theta_list.append(acc_theta)

"""## Plot Regret and Loss"""

m = 0
for alg in algs:
  # plot
  cum_regret = regret_list[m].cumsum(axis=0)
  plt.plot(step, cum_regret.mean(axis=1),
    alg[2], dashes=linestyle2dashes(alg[3]), label=alg[4])
  plt.errorbar(step[sube], cum_regret[sube, :].mean(axis=1),
    cum_regret[sube, :].std(axis=1) / np.sqrt(cum_regret.shape[1]),
    fmt="none", ecolor=alg[2])
  m += 1

plt.title("(a) Linear bandit")
plt.xlabel("Round n")
plt.ylabel("Regret")
# plt.ylim(0, 1000)
plt.legend(loc="upper left", ncol=2, frameon=False)

plt.tight_layout()
plt.savefig("Linear_reg.pdf", format="pdf", dpi=1200, bbox_inches=0)
plt.savefig("Linear_reg.png", format="png", dpi=1200, bbox_inches=0)
plt.show()

plt.clf()
plt.cla()
plt.close()

m = 0
for alg in algs:
  # plot
  cum_acc = acc_list[m].cumsum(axis=0)
  plt.plot(step, cum_acc.mean(axis=1),
    alg[2], dashes=linestyle2dashes(alg[3]), label=alg[4])
  plt.errorbar(step[sube], cum_acc[sube, :].mean(axis=1),
    cum_acc[sube, :].std(axis=1) / np.sqrt(cum_acc.shape[1]),
    fmt="none", ecolor=alg[2])
  m += 1

plt.title("(b) Linear bandit")
plt.xlabel("Round n")
plt.ylabel("Loss")
# plt.ylim(0, 3000)
plt.legend(loc="upper left", ncol=2, frameon=False)

plt.tight_layout()
plt.savefig("Linear_acc.pdf", format="pdf", dpi=1200, bbox_inches=0)
plt.savefig("Linear_acc.png", format="png", dpi=1200, bbox_inches=0)
plt.show()

plt.clf()
plt.cla()
plt.close()

m = 0
for alg in algs:
  # plot
  cum_acc_theta = acc_theta_list[m].cumsum(axis=0)
  plt.plot(step, cum_acc_theta.mean(axis=1),
    alg[2], dashes=linestyle2dashes(alg[3]), label=alg[4])
  plt.errorbar(step[sube], cum_acc_theta[sube, :].mean(axis=1),
    cum_acc_theta[sube, :].std(axis=1) / np.sqrt(cum_acc_theta.shape[1]),
    fmt="none", ecolor=alg[2])
  m += 1

plt.title("(c) Linear bandit")
plt.xlabel("Round n")
plt.ylabel("Loss (theta)")
plt.ylim(0, 4000)
plt.legend(loc="upper left", ncol=2, frameon=False)

plt.tight_layout()
plt.savefig("Linear_acc_theta.pdf", format="pdf", dpi=1200, bbox_inches=0)
plt.savefig("Linear_acc_theta.png", format="png", dpi=1200, bbox_inches=0)
plt.show()

plt.clf()
plt.cla()
plt.close()

# algs = [
#   ("LinGreedy", {}, "blue", "-", r"$\varepsilon$-greedy ($\varepsilon$ = $\delta$ / 10)"),
#   ("LinExploreCommit", {}, "cyan", "-", r"EtC ($\varepsilon$ = $\delta$ / 10)"),
#   ("LinPhasedElim", {}, "orange", "-", "Elimination"),
#   ("CODE", {}, "red", "-", "CODE")]
# deltas = [0.9, 0.75, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01]

# mpl.rcParams["font.size"] = 7
# plt.figure(figsize=(3.4, 2.25))

# # simulation
# for alg in algs:
#   regret = np.zeros((len(deltas), num_runs))
#   for ndx in range(len(deltas)):
#     delta = deltas[ndx]

#     # all runs for a single algorithm
#     alg_class = globals()[alg[0]]
#     if alg[0] in ["LinGreedy", "LinExploreCommit"]:
#       alg[1]["epsilon"] = delta / 10
#     else:
#       alg[1]["delta"] = delta
#     subregret, _ = evaluate(alg_class, alg[1], envs, n)
#     regret[ndx, :] = subregret.sum(axis=0)

#   plt.semilogx(deltas, regret.mean(axis=-1),
#     alg[2], dashes=linestyle2dashes(alg[3]), label=alg[4])
#   plt.errorbar(deltas, regret.mean(axis=-1),
#     regret.std(axis=-1) / np.sqrt(regret.shape[-1]),
#     fmt="none", ecolor=alg[2])

# plt.title("(b) Linear bandit tuning")
# plt.xlabel(r"Failure probability $\delta$")
# plt.ylabel("Regret")
# plt.ylim(0, 3000)
# plt.legend(loc="upper left", frameon=False)

# plt.tight_layout()
# plt.savefig("LinearTuning.pdf", format="pdf", dpi=1200, bbox_inches=0)
# plt.show()